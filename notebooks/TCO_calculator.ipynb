{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3e4c571-624d-4db6-b4d9-ae912879967b",
   "metadata": {},
   "source": [
    "# AIPerf -> LLM TCO Calculator Data Connector\n",
    "\n",
    "This notebook shows you how to do LLM performance benchmarking with the NVIDIA AIPerf tool and then export the data to a TCO (Total Cost of Ownership) calculator in [Excel spreadsheet format](./LLM_TCO_Calculator.xlsx).\n",
    "\n",
    "\n",
    "To execute this notebook, you can use the NVIDIA Triton server container:\n",
    "```\n",
    "docker run --gpus=all --ipc=host --net=host --rm -it -v $PWD:/myworkspace nvcr.io/nvidia/tritonserver:25.09-py3-sdk bash  \n",
    "```\n",
    "\n",
    "Then from within the docker interactive session:\n",
    "```\n",
    "pip install jupyterlab\n",
    "jupyter lab --ip 0.0.0.0 --port=8888 --allow-root --notebook-dir=/myworkspace\n",
    "```\n",
    "\n",
    "First, we define some metadata fields describing the deployment environment.\n",
    "\n",
    "**Notes:**\n",
    "- NIM engine ID  provides both the backend type (e.g. TensorRT-LLM, vLLM or SGlang) and precision. You can find this information when the NIM container starts.\n",
    "\n",
    "- This notebook collects data corresponding to a single deployment environment described by the metadata field. In this tutorial, we will make use of the `Meta-Llama-3-8B-Instruct` model. Note that NVIDIA NGC and HuggingFace model hub use slightly different identifier for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c18473-09ea-4a6f-87fa-d67fa3f7daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_field = {\n",
    " 'Model': \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    " 'GPU Type': \"H100_80GB\",\n",
    " 'number_of_gpus': 1,\n",
    " 'Precision': \"BF16\",\n",
    " 'Execution Mode': \"NIM-TRTLLM\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3df53-c103-4de2-81f5-419aa4d65f83",
   "metadata": {},
   "source": [
    "## Pre-requisite\n",
    "\n",
    "First, we install the AIPerf tool in the Pytorch container. \n",
    "As a client-side LLM-focused benchmarking tool, NVIDIA AIPerf provides key metrics such as time to first token (TTFT), inter-token latency (ITL), tokens per second (TPS), requests per second (RPS) and more. AIPerf also supports any LLM inference service conforming to the OpenAI API specification, a widely accepted de facto standard in the industry. For this benchmarking guide, we’ll use NVIDIA NIM, a collection of inference microservices that offer high-throughput and low-latency inference for both base and fine-tuned LLMs. NIM features ease-of-use and enterprise-grade security and manageability. \n",
    "\n",
    "### Install AIPerf tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5de6fe-8547-4259-956a-980aa8b71dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiperf\n",
      "  Downloading aiperf-0.2.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting aiofiles~=24.1.0 (from aiperf)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp~=3.12.14 (from aiperf)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting cyclopts<4,>=3 (from aiperf)\n",
      "  Downloading cyclopts-3.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting ffmpeg-python~=0.2.0 (from aiperf)\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.12/dist-packages (from aiperf) (1.26.4)\n",
      "Collecting openai~=1.92.2 (from openai[aiohttp]~=1.92.2->aiperf)\n",
      "  Downloading openai-1.92.3-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: orjson~=3.10.18 in /usr/local/lib/python3.12/dist-packages (from aiperf) (3.10.18)\n",
      "Collecting pillow~=11.1.0 (from aiperf)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: prometheus-client~=0.23.1 in /usr/local/lib/python3.12/dist-packages (from aiperf) (0.23.1)\n",
      "Collecting psutil~=7.0.0 (from aiperf)\n",
      "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting pydantic-settings~=2.10.0 (from aiperf)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic~=2.11.4 (from aiperf)\n",
      "  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyzmq~=26.4.0 (from aiperf)\n",
      "  Downloading pyzmq-26.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting rich~=14.1.0 (from aiperf)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel-yaml~=0.18.12 (from aiperf)\n",
      "  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting setproctitle~=1.3.6 (from aiperf)\n",
      "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: soundfile~=0.13.1 in /usr/local/lib/python3.12/dist-packages (from aiperf) (0.13.1)\n",
      "Collecting textual~=5.3.0 (from aiperf)\n",
      "  Downloading textual-5.3.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from aiperf) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.52.0 in /usr/local/lib/python3.12/dist-packages (from aiperf) (4.52.2)\n",
      "Collecting uvloop~=0.21.0 (from aiperf)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.12.14->aiperf) (2.6.1)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp~=3.12.14->aiperf)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.12.14->aiperf) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.12.14->aiperf) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.12.14->aiperf) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.12.14->aiperf) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.12.14->aiperf) (1.20.0)\n",
      "Collecting docstring-parser>=0.15 (from cyclopts<4,>=3->aiperf)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich-rst<2.0.0,>=1.3.1 (from cyclopts<4,>=3->aiperf)\n",
      "  Downloading rich_rst-1.3.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting future (from ffmpeg-python~=0.2.0->aiperf)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf)\n",
      "  Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (4.13.2)\n",
      "Collecting httpx-aiohttp>=0.1.6 (from openai[aiohttp]~=1.92.2->aiperf)\n",
      "  Downloading httpx_aiohttp-0.1.9-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic~=2.11.4->aiperf)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic~=2.11.4->aiperf)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic~=2.11.4->aiperf)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings~=2.10.0->aiperf)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich~=14.1.0->aiperf) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich~=14.1.0->aiperf) (2.19.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml~=0.18.12->aiperf)\n",
      "  Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile~=0.13.1->aiperf) (1.17.1)\n",
      "Requirement already satisfied: platformdirs<5,>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from textual~=5.3.0->aiperf) (4.5.0)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich~=14.1.0->aiperf)\n",
      "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.0->aiperf) (0.5.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (3.10)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile~=0.13.1->aiperf) (2.22)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.92.2->openai[aiohttp]~=1.92.2->aiperf) (0.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.52.0->aiperf) (2025.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich~=14.1.0->aiperf) (0.1.2)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify,plugins]>=2.1.0->textual~=5.3.0->aiperf)\n",
      "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting mdit-py-plugins (from markdown-it-py[linkify,plugins]>=2.1.0->textual~=5.3.0->aiperf)\n",
      "  Downloading mdit_py_plugins-0.5.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting docutils (from rich-rst<2.0.0,>=1.3.1->cyclopts<4,>=3->aiperf)\n",
      "  Downloading docutils-0.22.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.52.0->aiperf) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.52.0->aiperf) (2.4.0)\n",
      "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual~=5.3.0->aiperf)\n",
      "  Downloading uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Downloading aiperf-0.2.0-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cyclopts-3.24.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading openai-1.92.3-py3-none-any.whl (753 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzmq-26.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (855 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m855.9/855.9 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
      "Downloading textual-5.3.0-py3-none-any.whl (702 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.7/702.7 kB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading httpx_aiohttp-0.1.9-py3-none-any.whl (6.2 kB)\n",
      "Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.8/358.8 kB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading rich_rst-1.3.2-py3-none-any.whl (12 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (753 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Downloading docutils-0.22.2-py3-none-any.whl (632 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdit_py_plugins-0.5.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: uvloop, uc-micro-py, typing-inspection, setproctitle, ruamel.yaml.clib, pyzmq, python-dotenv, pygments, pydantic-core, psutil, pillow, jiter, future, docutils, docstring-parser, annotated-types, aiosignal, aiofiles, ruamel-yaml, rich, pydantic, mdit-py-plugins, linkify-it-py, ffmpeg-python, aiohttp, rich-rst, pydantic-settings, openai, httpx-aiohttp, textual, cyclopts, aiperf\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 27.1.0\n",
      "    Uninstalling pyzmq-27.1.0:\n",
      "      Successfully uninstalled pyzmq-27.1.0\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.19.1\n",
      "    Uninstalling Pygments-2.19.1:\n",
      "      Successfully uninstalled Pygments-2.19.1\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 7.1.3\n",
      "    Uninstalling psutil-7.1.3:\n",
      "      Successfully uninstalled psutil-7.1.3\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.2.1\n",
      "    Uninstalling pillow-11.2.1:\n",
      "      Successfully uninstalled pillow-11.2.1\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.2\n",
      "    Uninstalling aiosignal-1.3.2:\n",
      "      Successfully uninstalled aiosignal-1.3.2\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 14.0.0\n",
      "    Uninstalling rich-14.0.0:\n",
      "      Successfully uninstalled rich-14.0.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.18\n",
      "    Uninstalling aiohttp-3.11.18:\n",
      "      Successfully uninstalled aiohttp-3.11.18\n",
      "Successfully installed aiofiles-24.1.0 aiohttp-3.12.15 aiosignal-1.4.0 aiperf-0.2.0 annotated-types-0.7.0 cyclopts-3.24.0 docstring-parser-0.17.0 docutils-0.22.2 ffmpeg-python-0.2.0 future-1.0.0 httpx-aiohttp-0.1.9 jiter-0.11.1 linkify-it-py-2.0.3 mdit-py-plugins-0.5.0 openai-1.92.3 pillow-11.1.0 psutil-7.0.0 pydantic-2.11.10 pydantic-core-2.33.2 pydantic-settings-2.10.1 pygments-2.19.2 python-dotenv-1.2.1 pyzmq-26.4.0 rich-14.1.0 rich-rst-1.3.2 ruamel-yaml-0.18.16 ruamel.yaml.clib-0.2.14 setproctitle-1.3.7 textual-5.3.0 typing-inspection-0.4.2 uc-micro-py-1.0.3 uvloop-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install aiperf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6351a6-a5a3-4067-831e-abe26ae53969",
   "metadata": {},
   "source": [
    "### Setting up a NIM LLM server (optional)\n",
    "\n",
    "If you don't already have a target for benchmarking, like an OpenAI compatible LLM service, let's setup one. \n",
    "\n",
    "NVIDIA NIM provides the easiest and quickest way to put LLMs and other AI foundation models into production. Read [A Simple Guide to Deploying Generative AI with NVIDIA NIM](https://developer.nvidia.com/blog/a-simple-guide-to-deploying-generative-ai-with-nvidia-nim/) or consult the latest [NIM LLM documentation](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) to get started, which will walk you through hardware requirements and prerequisites, including NVIDIA NGC API keys.\n",
    "\n",
    "For convenience, the following commands have been provided for deploying NIM and executing inference from the [Getting Started Guide](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html):   \n",
    "\n",
    "                                                                                                    \n",
    "```\n",
    "export NGC_API_KEY=<YOUR_NGC_API_KEY> \n",
    "export LOCAL_NIM_CACHE=~/.cache/nim\n",
    "\n",
    "mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "\n",
    "docker run -it --rm \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 8000:8000 \\\n",
    "    nvcr.io/nim/meta/llama3-8b-instruct:latest\n",
    "```\n",
    "\n",
    "\n",
    "## Performance benchmarking script\n",
    "\n",
    "The next step is to define the use cases (i.e. input/output sequence length scenarios) and carry out the benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8395733-ce18-4447-845c-b3579acc2067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting benchmark.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "declare -A useCases\n",
    "\n",
    "# Populate the array with use case descriptions and their specified input/output lengths\n",
    "useCases[\"Translation\"]=\"200/200\"\n",
    "useCases[\"Text classification\"]=\"200/5\"\n",
    "useCases[\"Text summary\"]=\"1000/200\"\n",
    "\n",
    "# Function to execute AIPerf with the input/output lengths as arguments\n",
    "runBenchmark() {\n",
    "   local description=\"$1\"\n",
    "   local lengths=\"${useCases[$description]}\"\n",
    "   IFS='/' read -r inputLength outputLength <<< \"$lengths\"\n",
    "\n",
    "   echo \"Running AIPerf for $description with input length $inputLength and output length $outputLength\"\n",
    "   #Runs\n",
    "   for concurrency in 1 2 5 10 50 100 250; do\n",
    "\n",
    "       local INPUT_SEQUENCE_LENGTH=$inputLength\n",
    "       local INPUT_SEQUENCE_STD=0\n",
    "       local OUTPUT_SEQUENCE_LENGTH=$outputLength\n",
    "       local CONCURRENCY=$concurrency\n",
    "       local REQUEST_COUNT=$(($CONCURRENCY * 3))\n",
    "       local MODEL=meta/llama3-8b-instruct\n",
    "\n",
    "       aiperf profile \\\n",
    "           -m $MODEL \\\n",
    "           --endpoint-type chat \\\n",
    "           --streaming \\\n",
    "           -u localhost:8000 \\\n",
    "           --synthetic-input-tokens-mean $INPUT_SEQUENCE_LENGTH \\\n",
    "           --synthetic-input-tokens-stddev $INPUT_SEQUENCE_STD \\\n",
    "           --concurrency $CONCURRENCY \\\n",
    "           --request-count $REQUEST_COUNT \\\n",
    "           --output-tokens-mean $OUTPUT_SEQUENCE_LENGTH \\\n",
    "           --extra-inputs min_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "           --extra-inputs ignore_eos:true \\\n",
    "           --tokenizer meta-llama/Meta-Llama-3-8B-Instruct \\\n",
    "           --artifact-dir artifact/ISL${INPUT_SEQUENCE_LENGTH}_OSL${OUTPUT_SEQUENCE_LENGTH}/CON${CONCURRENCY}\n",
    "\n",
    "   done\n",
    "}\n",
    "\n",
    "# Iterate over all defined use cases and run the benchmark script for each\n",
    "for description in \"${!useCases[@]}\"; do\n",
    "   runBenchmark \"$description\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f1941-5206-4bca-a547-028e0ea50f21",
   "metadata": {},
   "source": [
    "This test will use the llama-3 tokenizer from HuggingFace, which is a guarded repository [https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct). You will need to apply for access, then login with your HF credential.\n",
    "\n",
    "Open a terminal in you Jupyter lab interface, then login to HF:\n",
    "```\n",
    "   pip install huggingface_hub\n",
    "   huggingface-cli login\n",
    "```\n",
    "\n",
    "Next, we execute the bash script, which will carry out the defined benchmarking scenarios and gather the data in a default directory named `artifacts` under the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfacd3-5755-4c0b-ae23-3abffceebbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash benchmark.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480b28d-6816-4c84-9124-bdc56fc81f41",
   "metadata": {},
   "source": [
    "## Reading AIPerf data\n",
    "\n",
    "Once performance benchmarking is done, we read and collect the results in a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff69c986-0c9b-46a9-8f28-800cd61ab24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "ISL_OSL_LIST = [\"200_5\", \"200_200\", \"1000_200\"]\n",
    "CONCURRENCIES  = [1, 2, 5, 10, 50, 100, 250]\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for concurrency in CONCURRENCIES :\n",
    "    for isl_osl in ISL_OSL_LIST:\n",
    "        ISL=isl_osl.split(\"_\")[0]\n",
    "        OSL=isl_osl.split(\"_\")[1]\n",
    "        \n",
    "        with open(f'./artifact/ISL{ISL}_OSL{OSL}/CON{concurrency}/profile_export_aiperf.json', 'r') as f:\n",
    "           data = json.load(f)\n",
    "             \n",
    "        row =  {\n",
    "         'Inter Token 90th Percentile Latency (ms)': data[\"records\"][\"inter_token_latency\"][\"p90\"],\n",
    "         'Inter Token 99th Percentile Latency (ms)': data[\"records\"][\"inter_token_latency\"][\"p99\"],\n",
    "         'Inter Token Average Latency (ms)': data[\"records\"][\"inter_token_latency\"][\"avg\"],\n",
    "         'Time to First Token 90th Percentile Latency (ms)': data[\"records\"][\"ttft\"][\"p90\"],\n",
    "         'Time to First Token 99th Percentile Latency (ms)': data[\"records\"][\"ttft\"][\"p99\"],\n",
    "         'Time to First Token Average Latency (ms)': data[\"records\"][\"ttft\"][\"avg\"],\n",
    "         'Request 90th Percentile Latency (ms)': data[\"records\"][\"request_latency\"][\"p90\"],\n",
    "         'Request 99th Percentile Latency (ms)': data[\"records\"][\"request_latency\"][\"p99\"],\n",
    "         'Request Latency (ms)': data[\"records\"][\"request_latency\"][\"avg\"],\n",
    "         'Requests per Second': data[\"records\"][\"request_throughput\"][\"avg\"],\n",
    "         'Tokens per Second': data[\"records\"][\"output_token_throughput\"][\"avg\"],\n",
    "         'Seq Length (ISL/OSL)': isl_osl,\n",
    "         'Concurrency': concurrency\n",
    "        } \n",
    "        \n",
    "        row = meta_field | row\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a997b59-3d4e-4877-953c-088563aa8998",
   "metadata": {},
   "source": [
    "## Exporting data to excel format\n",
    "\n",
    "We next export the benchmarking data to a NIM TCO Calculator compatible format, which comprises both metadata fields as well as performance metric fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f39710a9-882c-44aa-b428-d7ed2976eb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>GPU Type</th>\n",
       "      <th>number_of_gpus</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Execution Mode</th>\n",
       "      <th>Inter Token 90th Percentile Latency (ms)</th>\n",
       "      <th>Inter Token 99th Percentile Latency (ms)</th>\n",
       "      <th>Inter Token Average Latency (ms)</th>\n",
       "      <th>Time to First Token 90th Percentile Latency (ms)</th>\n",
       "      <th>Time to First Token 99th Percentile Latency (ms)</th>\n",
       "      <th>Time to First Token Average Latency (ms)</th>\n",
       "      <th>Request 90th Percentile Latency (ms)</th>\n",
       "      <th>Request 99th Percentile Latency (ms)</th>\n",
       "      <th>Request Latency (ms)</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Seq Length (ISL/OSL)</th>\n",
       "      <th>Concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>H100_80GB</td>\n",
       "      <td>1</td>\n",
       "      <td>BF16</td>\n",
       "      <td>NIM-TRTLLM</td>\n",
       "      <td>4.949345</td>\n",
       "      <td>4.967092</td>\n",
       "      <td>4.867871</td>\n",
       "      <td>20.532956</td>\n",
       "      <td>20.988718</td>\n",
       "      <td>18.889364</td>\n",
       "      <td>39.753273</td>\n",
       "      <td>40.061448</td>\n",
       "      <td>38.360850</td>\n",
       "      <td>25.419948</td>\n",
       "      <td>127.099742</td>\n",
       "      <td>200_5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>H100_80GB</td>\n",
       "      <td>1</td>\n",
       "      <td>BF16</td>\n",
       "      <td>NIM-TRTLLM</td>\n",
       "      <td>4.873248</td>\n",
       "      <td>4.874338</td>\n",
       "      <td>4.869679</td>\n",
       "      <td>20.860838</td>\n",
       "      <td>21.591705</td>\n",
       "      <td>18.887994</td>\n",
       "      <td>990.637183</td>\n",
       "      <td>991.584959</td>\n",
       "      <td>987.954030</td>\n",
       "      <td>1.010877</td>\n",
       "      <td>202.175430</td>\n",
       "      <td>200_200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>H100_80GB</td>\n",
       "      <td>1</td>\n",
       "      <td>BF16</td>\n",
       "      <td>NIM-TRTLLM</td>\n",
       "      <td>4.708417</td>\n",
       "      <td>4.711556</td>\n",
       "      <td>4.700263</td>\n",
       "      <td>46.373498</td>\n",
       "      <td>46.790232</td>\n",
       "      <td>45.210619</td>\n",
       "      <td>981.959344</td>\n",
       "      <td>982.167204</td>\n",
       "      <td>980.562975</td>\n",
       "      <td>1.018414</td>\n",
       "      <td>203.682855</td>\n",
       "      <td>1000_200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>H100_80GB</td>\n",
       "      <td>1</td>\n",
       "      <td>BF16</td>\n",
       "      <td>NIM-TRTLLM</td>\n",
       "      <td>6.257329</td>\n",
       "      <td>6.340666</td>\n",
       "      <td>5.838733</td>\n",
       "      <td>29.911478</td>\n",
       "      <td>33.787478</td>\n",
       "      <td>22.652078</td>\n",
       "      <td>53.725809</td>\n",
       "      <td>57.137345</td>\n",
       "      <td>46.007011</td>\n",
       "      <td>41.722412</td>\n",
       "      <td>208.612062</td>\n",
       "      <td>200_5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>H100_80GB</td>\n",
       "      <td>1</td>\n",
       "      <td>BF16</td>\n",
       "      <td>NIM-TRTLLM</td>\n",
       "      <td>4.920428</td>\n",
       "      <td>4.921603</td>\n",
       "      <td>4.912614</td>\n",
       "      <td>29.580081</td>\n",
       "      <td>33.868314</td>\n",
       "      <td>22.379320</td>\n",
       "      <td>1007.499842</td>\n",
       "      <td>1011.103446</td>\n",
       "      <td>999.989505</td>\n",
       "      <td>1.995674</td>\n",
       "      <td>399.134707</td>\n",
       "      <td>200_200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model   GPU Type  number_of_gpus Precision  \\\n",
       "0  meta-llama/Meta-Llama-3-8B-Instruct  H100_80GB               1      BF16   \n",
       "1  meta-llama/Meta-Llama-3-8B-Instruct  H100_80GB               1      BF16   \n",
       "2  meta-llama/Meta-Llama-3-8B-Instruct  H100_80GB               1      BF16   \n",
       "3  meta-llama/Meta-Llama-3-8B-Instruct  H100_80GB               1      BF16   \n",
       "4  meta-llama/Meta-Llama-3-8B-Instruct  H100_80GB               1      BF16   \n",
       "\n",
       "  Execution Mode  Inter Token 90th Percentile Latency (ms)  \\\n",
       "0     NIM-TRTLLM                                  4.949345   \n",
       "1     NIM-TRTLLM                                  4.873248   \n",
       "2     NIM-TRTLLM                                  4.708417   \n",
       "3     NIM-TRTLLM                                  6.257329   \n",
       "4     NIM-TRTLLM                                  4.920428   \n",
       "\n",
       "   Inter Token 99th Percentile Latency (ms)  Inter Token Average Latency (ms)  \\\n",
       "0                                  4.967092                          4.867871   \n",
       "1                                  4.874338                          4.869679   \n",
       "2                                  4.711556                          4.700263   \n",
       "3                                  6.340666                          5.838733   \n",
       "4                                  4.921603                          4.912614   \n",
       "\n",
       "   Time to First Token 90th Percentile Latency (ms)  \\\n",
       "0                                         20.532956   \n",
       "1                                         20.860838   \n",
       "2                                         46.373498   \n",
       "3                                         29.911478   \n",
       "4                                         29.580081   \n",
       "\n",
       "   Time to First Token 99th Percentile Latency (ms)  \\\n",
       "0                                         20.988718   \n",
       "1                                         21.591705   \n",
       "2                                         46.790232   \n",
       "3                                         33.787478   \n",
       "4                                         33.868314   \n",
       "\n",
       "   Time to First Token Average Latency (ms)  \\\n",
       "0                                 18.889364   \n",
       "1                                 18.887994   \n",
       "2                                 45.210619   \n",
       "3                                 22.652078   \n",
       "4                                 22.379320   \n",
       "\n",
       "   Request 90th Percentile Latency (ms)  Request 99th Percentile Latency (ms)  \\\n",
       "0                             39.753273                             40.061448   \n",
       "1                            990.637183                            991.584959   \n",
       "2                            981.959344                            982.167204   \n",
       "3                             53.725809                             57.137345   \n",
       "4                           1007.499842                           1011.103446   \n",
       "\n",
       "   Request Latency (ms)  Requests per Second  Tokens per Second  \\\n",
       "0             38.360850            25.419948         127.099742   \n",
       "1            987.954030             1.010877         202.175430   \n",
       "2            980.562975             1.018414         203.682855   \n",
       "3             46.007011            41.722412         208.612062   \n",
       "4            999.989505             1.995674         399.134707   \n",
       "\n",
       "  Seq Length (ISL/OSL)  Concurrency  \n",
       "0                200_5            1  \n",
       "1              200_200            1  \n",
       "2             1000_200            1  \n",
       "3                200_5            2  \n",
       "4              200_200            2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5baf8e86-c8d1-42fc-94d3-15b592a5adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "125f78e6-cc51-4091-bb16-9a1d8403d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    " 'Model',\n",
    " 'GPU Type',\n",
    " 'Seq Length (ISL/OSL)',\n",
    " 'number_of_gpus',\n",
    " 'Concurrency',\n",
    " 'Precision',\n",
    " 'Execution Mode',\n",
    " 'Inter Token 90th Percentile Latency (ms)',\n",
    " 'Inter Token 99th Percentile Latency (ms)',\n",
    " 'Inter Token Average Latency (ms)',\n",
    " 'Time to First Token 90th Percentile Latency (ms)',\n",
    " 'Time to First Token 99th Percentile Latency (ms)',\n",
    " 'Time to First Token Average Latency (ms)',\n",
    " 'Request 90th Percentile Latency (ms)',\n",
    " 'Request 99th Percentile Latency (ms)',\n",
    " 'Request Latency (ms)',\n",
    " 'Requests per Second',\n",
    " 'Tokens per Second'\n",
    " ]\n",
    "df[columns].to_excel('data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc138b-6d92-49aa-a9a6-3ad31ad75c87",
   "metadata": {},
   "source": [
    "## Importing the data to the TCO calculator\n",
    "\n",
    "The [NIM TCO calculator tool](LLM_TCO_Calculator.xlsx) is implemented as an Excel spreadsheet. You can use MS Excel spreadsheet to open the excel file above, then simply copy the data rows into the \"data\" subsheet of the TCO calculator. That will complete the import phase and make the new data available in the TCO calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1645aa-28e9-45d1-9db9-b86af239e627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

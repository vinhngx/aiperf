# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

from aiperf.common.enums import GenericMetricUnit, MetricFlags
from aiperf.common.exceptions import NoMetricValue
from aiperf.common.models import ParsedResponseRecord
from aiperf.metrics import BaseRecordMetric
from aiperf.metrics.derived_sum_metric import DerivedSumMetric
from aiperf.metrics.metric_dicts import MetricRecordDict


class OutputSequenceLengthMetric(BaseRecordMetric[int]):
    """
    Post-processor for calculating Output Sequence Length (OSL) metrics from records.

    This is the total number of tokens generated by the model for one individual record.
    It is sometimes referred to as "Completion Token Count". It includes both reasoning and
    output tokens.

    Formula:
        Output Sequence Length = Output Token Count + Reasoning Token Count
    """

    tag = "output_sequence_length"
    header = "Output Sequence Length"
    short_header = "OSL"
    unit = GenericMetricUnit.TOKENS
    display_order = 600
    flags = MetricFlags.PRODUCES_TOKENS_ONLY | MetricFlags.LARGER_IS_BETTER
    required_metrics = None

    def _parse_record(
        self,
        record: ParsedResponseRecord,
        record_metrics: MetricRecordDict,
    ) -> int:
        """
        This method extracts the output and reasoning token counts from the record and returns the sum.

        Raises:
            ValueError: If the record does not have a output or reasoning token count.
        """
        if record.output_token_count is None and record.reasoning_token_count is None:
            raise NoMetricValue(
                "Output and reasoning token counts are missing in the record."
            )

        return (record.output_token_count or 0) + (record.reasoning_token_count or 0)


class BenchmarkTokenCountMetric(DerivedSumMetric[int, OutputSequenceLengthMetric]):
    """
    This is the total number of completion tokens processed by the benchmark.

    Formula:
        ```
        Benchmark Token Count = Sum(Output Sequence Lengths)
        ```
    """

    tag = "benchmark_token_count"
    header = "Benchmark Token Count"
    short_header = "Tokens"
    short_header_hide_unit = True
    flags = (
        MetricFlags.PRODUCES_TOKENS_ONLY
        | MetricFlags.LARGER_IS_BETTER
        | MetricFlags.HIDDEN
    )
